#import ("/src/titul.typ"): *
#import ("/src/preamble.typ"): *
#show: main
#titul(
  Институт: [Информационных Технологий],
  Кафедра: [Вычислительной техники],
  Практика: [Отчёт по практическим работам №1-3],
  Дисциплина: ["Математическое обеспечение СППР"],
  Группа: [ИКБО-04-22],
  Студент: [Егоров Л.А.],
  Должность: [старший преподаватель],
  Преподаватель: [Семёнов Р.Э.]
)
#show: template

#outline()

#heading("Введение", numbering: none)

Задача нахождения оптимального значения функции является одной из важнейших в машинном обучении, ведь большинство алгоритмов основываются на минимизации функции потерь.

Для решения задачи оптимизации могут применяться различные методы:
1. Методы нулевого порядка --- позволяют решить задачу оптимизации для произвольных функций, однако обладают невысокой сходимостью.
2. Методы первого порядка --- сходятся к глобальному минимуму выпуклых функций, однако для произвольных функций необходимо, чтобы в каждой точке существовала производная первого порядка. Также для произвольных функций нет гарантии, что метод сойдётся к глобальному минимуму вместо локального.
3. Методы второго порядка --- аналогичны метода первого порядка, однако используют производную второго порядка и обладают более высокой сходимостью.

= Методы нулевого порядка
== Теоретическая информация

Методы нулевого порядка выгодны тем, что не требуют вычисления производных и что им достаточно только непрерывности целевой функции. Однако сходимость у данных методов не доказана и является эвристической.

== Постановка задачи

Требуется найти точку локального минимума $overline(x^*) = (x_1^*, x_2^*)$ целевой функции от 2-х переменных (@func-1) на множестве допустимых значений.

$ 2x_1^2 - 2x_1x_2 + 3x_2^2 + x_1 - 3x_2 $<func-1>

В качестве метода оптимизации используется метод Хука-Дживса, который состоит из последовательности шагов исследующего поиска относительно базисной точки и поиска по образцу.

== Ручной расчёт оптимального значения функции

Перед реализацией алгоритма проведён ручной расчёт минимального значения функции, представленной Формулой @func-1. Для этого сначала проверено необходимое условие экстремума функции.

#eq-simple(
  $ cases(
     (partial f) / (partial x_1) = 0,
     (partial f) / (partial x_2) = 0
   ) <=> cases(
     4x_1 - 2x_2 + 1 = 0,
     -2x_1 + 6x_2 - 3 = 0
   ) <=> cases(
    x_1 = 0,
    x_2 = 0.5
  ) \ 
  $
)

Значение целевой функции в этой точке равно $2 dot 0 - 2 dot 0 dot 0.5 + 3 dot 0.5^2 + + 0 - 3 dot 0.5 = -0.75$.

Далее проверено достаточное условие экстремума функции. Для этого вычислены все производные второго порядка для целевой функции.

#eq-simple(
  $
    A = (partial^2 f) / (partial x_1^2) = 4 > 0 \
    B = (partial^2 f) / (partial x_1 partial x_2) = -2 \
    C = (partial^2 f) / (partial x_2^2) = 6 \
    A C - B^2 = 4 dot 6 - (-2)^2 = 28 > 0
  $
)

Следовательно, для любых допустимых значений аргументов выполняется достаточное условие экстремума, а значит, точка $(0, 0.5)$ является точкой минимума функции. При этом, у целевой функции больше нет стационарных точек, поэтому в точка достигается глобальный минимум функции.

График функции представлен на Рисунке @plot.

#figure(image("/img/math-report/math-report_2025-02-23-21-55-54.png"), caption: [График целевой функции])<plot>

На Рисунке @projection представлен график функции в проекции на плоскость XOZ.

#figure(image("/img/math-report/math-report_2025-06-20-12-38-48.png"), caption: [График целевой функции в проекции на плоскость XOZ])<projection>


== Ручной расчёт алгоритма

Зафиксированы следующие параметры алгоритма:
- начальная точка: $overline(x^((0))) = (1, 1)$
- точность приближения: $epsilon = 0.0001$
- коэффициент уменьшения шага: $d = 10$
- начальная величина шага: $h = 0.2$
- ускоряющий коэффициент: $m = 2$

=== Первая итерация

Значение целевой функции в базисной точке: $f(overline(x^((0)))) = 1$. После фиксации второй координаты $x_2^((0))$ и выполнения шага в положительном направлении $x_1^((0))$, получена точка $overline(x^((1))) = (x_1^((0)) + h, x_2^((0))) = (1.2, 1)$. Значение целевой функции в этой точке равно $f(overline(x^((1)))) = 1.68 > f(overline(x^((0))))$, следовательно, шаг в этом направлении считается неудачным.

Далее выполнен шаг в отрицательном направлении $x_1^((0))$ и получена точка $overline(x^((1))) = (x_1^((0)) - h, x_2^((0))) = (0.8, 1)$. Значение целевой функции в этой точке равно $f(overline(x^((1)))) = 0.48 < f(overline(x^((0))))$, следовательно, шаг в этом направлении считается удачным. Таким образом, фиксируется точка $overline(x^((1))) = (0.8, 1)$.

После выполнения шага в положительном направлении $x_2$, получена точка $(x_1^((1)), x_2^((1)) + h) = (0.8, 1.2)$. Значение целевой функции в этой точке равно $f(0.8, 1.2) = 0.88 > f(overline(x^((1))))$, следовательно, шаг в этом направлении считается неудачным.

Далее выполнен шаг в отрицательном направлении $x_2$ и получена точка $(x_1^((1)), x_2^((0)) - h) = (0.8, 0.8)$. Значение целевой функции в этой точке равно $f(overline(x^((1)))) = 0.32 < f(overline(x^((1))))$, следовательно, шаг в этом направлении считается удачным. Таким образом, фиксируется точка $overline(x^((1))) = (0.8, 0.8)$.

Поскольку $overline(x^((1))) != overline(x^((0)))$, то выполняется поиск по образцу:

#eq-simple(
  $
   overline(x^((p))) = overline(x^((1))) + m(overline(x^((1))) - overline(x^((0)))) = (0.8, 0.8) + 2[(0.8, 0.8) - (1, 1)] = (0.4, 0.4) \
   f(overline(x^((p)))) = -0.32
  $
)

== Программная реализация
На Рисунке @hooke-jeeves представлен результат работы метода Хука-Дживса. Для нахождения оптимального значения алгоритм провёл 15 итераций.
#figure(image("/img/math-report/math-report_2025-06-20-12-40-50.png", height: 30%), caption: [Результат работы метода Хука-Дживса])<hooke-jeeves>
Реализация метода Хука-Дживса представлена в Листинге @hooke-jeeves-code.

= Методы первого порядка
== Теоретическая информация
Методы первого порядка используют производные первого порядка, что позволяют гарантировать сходимость к глобальному минимуму выпуклых функций.

Координаты новой точки в данном методе вычисляются по Формуле @grad-formula.

$ overline(x^(k + 1)) = overline(x^k) - h_k Delta f(overline(x^k)) $<grad-formula>
#print_symbols(
  [k --- номер итерации],
  [$h_k$ --- величина шага. В данной работе принимается постоянной],
  [$Delta f(overline(x^k))$ --- градиент функции]
)

Если $f(overline(x^(k + 1))) > f(overline(x^k))$, то нужно уменьшить шаг в два раза.

Условие окончания поиска:
#eq-simple($ norm(Delta f(overline(x^k))) = sqrt(sum_(i=1)^n ((diff f (overline(x^k))) / (diff x_i))) <= epsilon $)

== Программная реализация
На Рисунке @gradient представлен результат работы алгоритма градиентного спуска. Для достижения глобального минимума алгоритму понадобилось 13 итераций.
#figure(image("/img/math-report/math-report_2025-06-20-12-53-58.png", height: 23%), caption: [Результат работы градиентного спуска])<gradient>
Реализация градиентного спуска представлена в Листинге @gradient-code.

= Методы второго порядка
== Теоретическая информация
Методы второго порядка используют производные второго порядка, что позволяют гарантировать сходимость к глобальному минимуму выпуклых функций.

Основным преимуществом методов второго порядка является их высокая сходимость - для квадратичных функций доказана сходимость за одну итерацию.

Координаты новой точки в данном методе вычисляются по Формуле @newton-formula.

$ overline(x^(k + 1)) = overline(x^k) + overline(p^k) $<newton-formula>
#print_symbols(
  [k --- номер итерации],
  [$p(overline(x^k)) = -H^(-1)(overline(x^k)) Delta f(overline(x^k))$ --- вектор направления спуска],
  [$H(overline(x^k))$ --- матрица Гессе (матрица вторых производных)]
)

Условие окончания поиска:
#eq-simple($ norm(Delta f(overline(x^k))) = sqrt(sum_(i=1)^n ((diff f (overline(x^k))) / (diff x_i))) <= epsilon $)

== Программная реализация
На Рисунке @newton представлен результат работы алгоритма градиентного спуска. Для достижения глобального минимума алгоритму понадобилась 1 итерация.
#figure(image("/img/math-report/math-report_2025-06-20-13-04-23.png"), caption: [Результат работа метода Ньютона])<newton>

Реализация метода Ньютона представлена в Листинге @newton-code.

#heading([Заключение], numbering: none)
После тестирования реализованных методов подтверждена теоретическая информация о них --- метод нулевого порядка прост в реализации, однако имеет низкую скорость сходимости (15 итераций). Метод первого порядка (градиентный спуск с дроблением шага) уже улучшил скорость сходимости (13 итераций), а самым быстрым оказался метод Ньютона, для которого удалось подтвердить, что он сходится к глобальному минимуму квадратичной функции за одну итерацию.

====== Реализация метода Хука-Дживса
#simple-code(
  raw(read("prac1/hooke_jeeves.py")),
  [Код реализации метода Хука-Дживса],
  label: <hooke-jeeves-code>
)
====== Реализация градиентного спуска с дроблением шага
#simple-code(
  raw(read("prac2/gradient.py")),
  [Код реализации градиентного спуска],
  label: <gradient-code>
)
====== Реализация метода Ньютона
#simple-code(
  raw(read("prac3/newton.py")),
  [Код реализации метода Ньютона],
  label: <newton-code>
)