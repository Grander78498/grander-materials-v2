#import "/src/preamble.typ": *
#import "/src/titul-holm.typ": *
#show: main
#titul(
  Институт: [Информационных Технологий],
  Кафедра: [Вычислительной техники],
  Практика: upper[Практические работы №2, 3, 6],
  Дисциплина: ["Проектирование интеллектуальных систем (Часть 1/2)"],
  Группа: [ИКБО-04-22],
  Студент: [Егоров Л.А.],
  Преподаватель: [Холмогоров В.В.]
)
#show: template

#let lb = linebreak(justify: true)

#outline()

#heading([Введение], numbering: none)
Сервисы для потокового воспроизведения музыки являются самыми используемыми вариантами для прослушивания аудио, обходя по востребованности физические носители. Одним из самых явных удобств данных сервисов является возможность подбора песен по набору ранее прослушанных, а также жанровые рекомендации. Поскольку алгоритмы, используемые для данных возможностей, не доступны в открытом доступе, принято решение самостоятельно провести анализ музыкальных композиций.

Для анализа нужно выполнить две задачи:
- кластеризация --- определение, к какой группе композиций можно отнести новую. Применение: предложение новых песен для уже существующего плейлиста
- классификация --- для определения жанровой принадлежности композиции

Самым распространнёным алгоритмом кластеризации является KMeans, однако своё применение имеет и алгоритм кластеризации, основанный на плотности распределения точек (DBSCAN), также как и ансамбль алгоритмов кластеризации.

Самым простым алгоритмом классификации является KNN, более эффективными являются решающие деревья и их объединения в ансамбли (в том числе, в случайный лес).

= Теоретический раздел
== Кластеризация
=== KMeans
Алгоритм минимизирует сумму внутрикластерных расстояний (@kmeans):

$ V = sum_(i=1)^m sum_(j=1)^n rho(x, mu_j) --> "min" $<kmeans>
#print_symbols(
  [$m$ --- количество данных во входной выборке],
  [$n$ --- количество кластеров],
  [$mu_j$ --- центроид $j$-го кластера],
  [$rho$ --- мера расстояния между точками. Обычно выбирается Евклидово расстояние, однако можно применять и другие метрики для получения отличающихся результатов]
)

=== DBSCAN
Алгоритм разделяет точки на группы, которые лежат друг от друга на большом расстоянии. Поэтому алгоритм сам способен определить количество кластеров, а также выделить шумовые точки на основе того, что они будут далеко находиться от других точек.

=== Ансамбль алгоритмов кластеризации
Алгоритм построения ансамбля:
1. Алгоритм KMeans вызывается нечётное количество раз, для каждого вызова используются разные параметры и/или разные метрики.
2. Для каждого из алгоритмов определяется его точность. Для этого может использоваться индекс Rand (Формула @rand-index) или число, обратное компактности кластеров (Формула @cluster-cohesion). В первом случае необходимо иметь метки реальных классов, для второго метода это не требуется.
3. На основе рассчитанной точности определяются веса каждого из алгоритмов (@cluster-weight):
$ omega_l = ("Acc"_l) / (sum_(l=1)^L "Acc"_l) $<cluster-weight>
#print_symbols(
  [$L$ --- количество алгоритмов в ансамбле],
  [$"Acc"_l$ --- точность $l$-го алгоритма]
)
4. Для каждого алгоритма составляется матрица сходства/различий (@diff-matrix):
$ H = {h_(i j)}, \ h_(i j) = cases(
  0 ", если элементы i и j попали в один кластер",
  1 ", если элементы i и j попали в разные кластеры"
) $<diff-matrix>
5. Все полученные матрицы складываются друг с другом с учётом полученных весов.
6. Итоговая матрица подаётся на вход иерархической агломеративной кластеризации.

=== Метрики кластеризации
В качестве метрик кластеризации могут использоваться:
1. Индекс Rand:
$ "Rand" = ("TP" + "TN") / ("TP" + "FP" + "FN" + "TN") $<rand-index>
#print_symbols(
  [TP --- количество пар, где элементы принадлежат одному кластеру и одному классу],
  [FP --- количество пар, где элементы принадлежат одному кластеру, но разным классам],
  [FN --- количество пар, где элементы принадлежат разным кластерам, но одному классу],
  [FN --- количество пар, где элементы принадлежат разным кластерам и разным классам],
)
2. Компактность кластеров:
$ "WSS" = sum_(i = 1)^M sum_(j = 1)^abs(C_j) (x_(i j) - overline(x_j)) $<cluster-cohesion>
#print_symbols(
  [M --- количество кластеров]
)

== Классификация
=== KNN

=== Дерево решений

=== Ансамбль на основе голосования

=== Бэггинг

=== Случайный лес

=== Метрики классификации
Используемые метрики классификации:
1. Аккуратность (@accuracy):
$ "Accuracy" = ("TP" + "TN") / ("TP" + "FP" + "FN" + "TN") $<accuracy>
2. Точность (@precision):
$ "Precision" = ("TP") / ("TP" + "FP") $<precision>
3. Полнота (@recall):
$ "Recall" = ("TP") / ("TP" + "FN") $<recall>
4. F1-мера (@f1-score):
$ "f1-score" = (2 times "precision" times "recall") / ("precision" + "recall") $<f1-score>
Также существуют micro-f1. macro-f1 и weighted-f1 для задач многоклассовой классификации.

== Расстояния
Используемые расстояния между объектами:
1. Евклидово расстояние (@euclid):
$ rho(x, y) = sqrt(sum_(i=1)^n (x_i - y_i)^2) $<euclid>
2. Манхэттенское расстояние (@manhattan):
$ rho(x, y) = sum_(i=1)^n abs(x_i - y_i) $<manhattan>
3. Расстояние Чебышева (@chebyshev):
$ rho(x, y) = max_(i=1)^n abs(x_i - y_i) $<chebyshev>
4. Коэффициент Жаккара (@jaccard):
$ K(x, y) = (sum_(i=1)^n x_i y_i) / (sum_(i=1)^n x_i^2 + sum_(i=1)^n y_i^2 - sum_(i=1)^n x_i y_i) $<jaccard>

== Описание данных
Для решения поставленных задач выбран набор данных с платформы Kaggle. Структура данных приведена в Таблице @fields.

#simple-table(
  header: ([Название поля], [Описание]),
  name: [Описание полей],
  label: <fields>,
  [track_id], [ID композиции из Spotify],
  [artists], [Исполнители композиции],
  [album_name], [Название альбома],
  [track_name], [Название композиции],
  [popularity], [Популярность композиции (от 0 до 100)],
  [duration_ms], [],
  [explicit], [],
  [danceability], [],
  [energy], [],
  [key], [],
  [loudness], [],
  [mode], [],
  [speechiness], [],
  [acousticness], [],
  [instrumentalness], [],
  [liveness], [],
  [valence], [],
  [tempo], [],
  [time_signature], [],
  [track_genre], [],
)

== Предобработка данных
Для предобработки выполнено несколько этапов:
1. Удалены дубликаты - те записи, у которых совпадают одновременно список исполнителей и название композиции.
2. Проведён корреляционный анализ - для этого выведена матрица корреляции (Рисунок @corr):
#figure(image("/img/holm-prac2/holm-prac2_2025-06-04-20-39-13.png"), caption: [Корреляционная матрица])<corr>
На основе данной матрицы принято решение убрать поля loudness и energy.
3. Выполнена нормализация данных с помощью StandardScaler, который использует Формулу @standard:
$ z_(i j) = (x_(i j) - mu_j) / sigma_j, i in [1, n], j in [1, m] $<standard>
#print_symbols(
  [n --- количество объектов в выборке],
  [m --- количество признаков],
  [$mu$ --- среднее значение среди всех объектов по одному признаку],
  [$sigma$ --- стандартное отклонение всех объектов по одному признаку]
)
4. Дополнительно, для решения задачи классификации, проведена обработка датасета алгоритмом DBSCAN - с помощью алгоритма помечаются и удаляются шумовые точки.

== Распределение данных

= Практический раздел
== Описание программных сущностей
Для каждого алгоритма написаны классы, реализующие их. У каждого класса есть метод fit, который принимает входные данные и обучает модель на их основе. У алгоритмов классификации также есть метод predict, который для поданных векторов выдаёт предсказания классов. Специфичные методы для каждого из классов представлены ниже:
1. Общие функции:
- preprocessing --- выполнение предобработки набора данных
- visualize --- отображение набора данных, преобразованного через метод главных компонент

2. KMeans:
- \_compute_distances --- расчёт расстояний между точкой и каждыми центрами кластеров
- \_assign_clusters --- расположение точки в кластер с ближайшим центром
- rand_index --- вычисление индекса Rand
- cluster_cohesion --- вычисление плотности кластера
- cluster_similarity_matrix --- расчёт матрицы сходства

3. DBSCAN:
- \_expand_cluster --- расширение кластера на основе плотности
- \_get_neighbors --- получение точек, входящих в окрестности данной точки
- denoise --- выдача индексов нешумовых точек

4. ClusterEnsemble:
- \_create_estimators --- инициализация моделей KMeans со случайно выбранными расстояниями

5. KNNClassifier: специфичные методы отсутствуют.

6. DecisionTreeClassifier:
- \_grow_tree --- построение дерева рекурсивным способом, от корня к листьям
- \_best_split --- нахождение лучшего разбиения в узле, т.е. выбор лучшего признака с лучшим пороговым значением
- \_information_gain --- нахождение прироста информации для текущего узла при выборе признака и порога
- \_gini_impurity --- вычисление индекса Джини
- \_entropy --- вычисление энтропии
- \_traverse_tree --- проход по дереву до листьев, используя пороги

7. Bagging --- реализация бэггинга на основе DecisionTreeClassifier. Специфичные методы отсутствуют
8. RandomForestClassifier --- библиотечная реализация случайного леса.

== Тестирование
=== Кластеризация
В Таблицу @test-cluster сведены данные о тестировании трёх алгоритмов кластеризации.

#simple-table(
  header: ([Название алгоритма], [Специфичные параметры], [Индекс Rand], [Плотность кластеров], [Время работы (мин:сек)]),
  name: [Тестирование алгоритмов кластеризации],
  label: <test-cluster>,
  [KMeans], [
    - количество кластеров = 6;
    - манхэттенское расстояние;
    - максимальное количество итераций: 300.
  ], [0.74478], [40315.875], [00:07],

  [DBSCAN], [
    - $epsilon = 2.1$;
    - min_samples = 8.
  ], [0.63134], [33871.4727], [01:55],

  [ClusterEnsemble], [
    - количество моделей = 13.
  ], [0.76585], [30696.6308], [02:42],
)
Для подбора оптимального количества моделей в ансамбле кластеризаторов проведено обучение нескольких ансамблей, с количеством моделей от 5 до 15 включительно (для каждого количества с учителем и без). График исследования представлен на Рисунке @cluster-n-model.

#figure(image("/img/holm-prac2/holm-prac2_2025-06-04-18-47-41.png"), caption: [Точность ансамбля в зависимости от количества моделей])<cluster-n-model>

Лучший результат обучения без учителя достигнут при количестве моделей, равном 13.

#figure(image("/img/holm-prac2/holm-prac2_2025-06-04-19-12-20.png"), caption: [Результат кластеризации ансамблем])<cluster-ensemble-result>

=== Классификация
В Таблицу @test-classes сведены данные о тестировании трёх алгоритмов кластеризации.

#simple-table(
  header: ([Название алгоритма], [Специфичные параметры], [Accuracy], [Macro-f1], [Время обучения (мин:сек)], [Время предсказания (мин:сек)]),
  name: [Тестирование алгоритмов кластеризации],
  label: <test-classes>,
  [KNN], [
    - количество ближайших соседей = 5;
    - манхэттенское расстояние;
    - максимальное количество итераций: 300.
  ], [0.63], [0.6], [00:00], [00:23],

  [Дерево решений], [
    - количество ближайших соседей = 5;
    - манхэттенское расстояние;
    - максимальное количество итераций: 300.
  ], [0.63], [0.6], [00:00], [00:23],

  [Бэггинг], [
    - количество ближайших соседей = 5;
    - манхэттенское расстояние;
    - максимальное количество итераций: 300.
  ], [0.63], [0.6], [00:00], [00:23],

  [Случайный лес], [
    - количество ближайших соседей = 5;
    - манхэттенское расстояние;
    - максимальное количество итераций: 300.
  ], [0.63], [0.6], [00:00], [00:23],
)

#heading([Заключение], numbering: none)

#bibliography("authors2.bib", full: true, style: "/src/gost-r-7-0-5-2008-numeric-alphabetical.csl", title: "Список использованных источников")

#appendix()

#set heading(offset: 5)
= Реализация алгоритма KMeans
#simple-code(raw(read("prac2/kmeans.py")), [Код файла kmeans.py], label: <kmeans-code>)
= Реализация алгоритма DBSCAN
#simple-code(raw(read("prac2/dbscan.py")), [Код файла dbscan.py], label: <dbscan-code>)
= Реализация ансамбля кластеризации
#simple-code(raw(read("prac2/cluster_ensemble.py")), [Код файла cluster_ensemble.py], label: <cluster-ensemble-code>)

= Реализация алгоритма KNN
#simple-code(raw(read("prac2/knn.py")), [Код файла knn.py], label: <knn-code>)
= Реализация дерева решений
#simple-code(raw(read("prac2/tree_classifier.py")), [Код файла tree_classifie.py], label: <tree_classifie-code>)
= Реализация бэггинга
#simple-code(raw(read("prac2/bagging.py")), [Код файла bagging.py], label: <bagging-code>)
= Реализация случайного леса
#simple-code(raw(read("prac2/random_forest.py")), [Код файла random_forest.py], label: <random-forest-code>)