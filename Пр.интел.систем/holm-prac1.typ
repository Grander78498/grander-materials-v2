#import "/src/preamble.typ": *
#import "/src/titul-holm.typ": *
#show: main
#titul(
  Институт: [Информационных Технологий],
  Кафедра: [Вычислительной техники],
  Практика: upper[Практическая работа №1],
  Дисциплина: ["Проектирование интеллектуальных систем (Часть 1/2)"],
  Группа: [ИКБО-04-22],
  Студент: [Егоров Л.А.],
  Преподаватель: [Холмогоров В.В.]
)
#show: template

#let lb = linebreak(justify: true)

#outline()

#heading([Введение], numbering: none)
Задача поиска ассоциативных правил заключается в нахождении зависимостей в транзакциях - насколько часто один товар берут в паре с другим товаром, насколько часто одно явление происходит зависимо от второго.

В рассматриваемой предметной области "Анализ любимых исполнителей" задача поиска ассоциативных правил позволяет создать систему, позволяющую рекомендовать пользователю новых исполнителей на основе уже выбранных любимых.

= Постановка задачи
Выбранная предметная область: "Анализ любимых исполнителей". В данной предметной области транзакциями являются множества любимых исполнителей у каждого пользователя.

Определена следующая цель --- реализовать алгоритмы поиска ассоциативных правил в рамках поставленной предметной области.

Поставлены следующие задачи:
- определить и изучить датасет, соотвествующий выбранной предметной области
- изучить алгоритмы поиска ассоциативных правил (Apriori, Eclat, #lb FP-Growth)
- написать программный код для реализации указанных алгоритмов
- сравнить основные показатели производительности алгоритмов: качество результатов, скорость работы и требуемое количество памяти

= Теоретическая часть

Для того, чтобы выявить ассоциативные правила, в первую очередь необходимо понимать, как определить качество этих правил. Для этого можно ввести следующие метрики (далее $A$ --- множество объектов, составляющих условие, $C$ --- множество объектов, составляющих следствие, $A -> C$ --- рассматриваемся ассоциация, $T$ --- множество всех транзакций):
1. *Поддержка*
Показывает вероятность встретить набор A и C в одной транзакции одновременно и высчитывается по Формуле @support.

$ s u p p(A -> C) = abs(A union C) / abs(T) $<support>

2. *Доверие*
Показывает, насколько часто правило оказывается верным, т.е. по сути является условной вероятностью, что в наборе окажется множество C, если в нём есть множество A. Вычисляется по Формуле @confidence.

$ "conf"(A -> C) = "supp"(A -> C) / "supp"(A) $<confidence>

3. *Достоверность (убеждённость)*
Показывает независимость A и C - чем больше значение, тем более зависимы. Вычисляется по Формуле @conviction.

$ "conv"(A -> C) = (1 - "sup"(C)) / (1 - "conf"(A -> C)) $<conviction>

4. *Лифт*
Также показывает, насколько независимы A и C друг от друга. Если лифт равен 1, то условие и следствие независимы, если больше 1, то зависимы, а если меньше 1, то наличие множества A в наборе имеет отрицательный эффект на присутствие второго объекта, и наоборот. Вычисляется по Формуле @lift.

$ "lift"(A -> C) = "supp"(A union C) / ("supp"(A) times "supp"(C)) $<lift>

5. *Рычаг*
Также показывает, насколько независимы A и C друг от друга. Вычисляется как разность вероятности встретить A и C одновременно и мат.ожидания A и C, как если бы они были независимы друг от друга (Формула @leve).

$ "leve"(A -> C) = "supp"(A union C) - "supp"(A) times "supp"(C) $<leve>

Алгоритмы поиска ассоциативных правил рассчитаны на ускорение перебора всех возможных наборов условий и следствий за счёт отбрасывания заведомо редких наборов. Поэтому наиболее применимой метрикой является поддержка, показывающая вероятность встретить набор во всём множестве транзакций.

== Алгоритм Apriori
Для данного алгоритма данные представляются в нормализованном виде, т.е. если в i-ой транзакции элемент _Item_ встречается, то в датасете в соответствующей ячейке будет стоять 1, и 0 в обратном случае.

Далее строится префиксное дерево - родителем каждого узла является набор, содержащий префикс данного набора без одного элемента. Для первого уровня просто отбираются те элементы, которые встречаются чаще заданного порога, далее каждый уровень заполняется, используя наборы из предыдущего.

Для генерации наборов k-го уровня объединяются попарно все наборы k - 1 уровня, которые имеют одинаковый префикс. Для полученного набора проверяется множество его подмножеств, состоящих из k - 1 элемента - если хотя бы одно такое подмножество встречается реже заданного порога, то набор отбрасывается.

Алгоритм выполняется до тех пор, пока есть возможность составлять набор более высоких уровней.
== Алгоритм Eclat
В основе данного алгоритма также лежит префиксное дерево, однако отличием от Apriori алгоритма является способ проверки частотности набора. Перед началом алгоритма для каждого уникального элемента составляется множество транзакций, в котором он встречается, затем отбрасываются те, которые встречают реже заданного порога.

Далее для составления набора более высокого уровня берутся попарно наборы с одинаковыми префиксами, и при объединении наборов пересекаются множества транзакций, в которых они встречаются. Таким образом, получается новое множество транзакций, и если его размер меньше порога, то набор отбрасывается.

Алгоритм выполняется до тех пор, пока есть возможность составлять набор более высоких уровней.
== Алгоритм FP-Growth
Алгоритм предполагает иной подход - вместо генерации кандидатов происходит сохранение всего множества транзакций в виде дерева, а затем с помощью обхода этого дерева выявляются часто встречающиеся наборы.

Структура узла данного дерева:
- значение
- количество повторений

Для составления дерева наборы объектов в каждой транзакции сортируются по убыванию частотности объектов во всём множестве транзакций. Затем инициализируется дерево с пустым корнем. Далее осуществляется проход по всем транзакциям, где для каждой транзакции выполняются следующие действия:

1. Если элемент не входит в список детей текущего узла, то он добавляется в него, иначе для подходящего узла увеличивается счётчик количества повторений на единицу
2. Выполняется переход к полученному узлу

Таким образом, на верхнем уровне дерева расположатся самые частые элементы, а листьями будут самые редкие.

Проход по дереву осуществляется по элементам в порядке возрастания их частотности. Для каждого элемента вычисляется, насколько часто другие элементы являются его предками (т.е. родителями, прародителями, прапрародителями...). После подсчёта в новый набор идут сам рассматриваемый элемент и те элементы, частота которых выше установленного порога.

== Генерация ассоциативных правил
Все рассмотренные выше алгоритмы предназначены для ускорения поиска часто встречающихся наборов, но генерация самих правил для них общая. 

Из каждого множества размером k рассматриваются все подмножества размером k - 1 --- эти подмножества становятся условием в правиле, а оставшийся элемент --- следствием. Уже для составленных правил выполняется отбор на основе, например, метрики доверия.

= Описание данных
== Генерация данных
Ввиду отсутствия в общем доступе информации от стриминговых сервисов о том, какие любимые исполнители есть у пользователей (или ввиду отсутствия у автора данного отчёта навыков по поиску такого датасета) решено сгенерировать искусственные данные.

Для этого выбрано 10 исполнителей из двух разных жанров (Таблица @performers).

#simple-table(header: ([Рок], [Хип-хоп]),
[Queen], [Kendrick Lamar],
[Scorpions], [Kanye West],
[Led Zeppelin], [Tyler, the Creator],
[Black Sabbath], [JAY-Z],
[Deep Purple], [2Pac],
name: [Выбранные исполнители],
label: <performers>
)

В каждом столбце элементы отсортированы по убыванию количества слушателей за месяц на стриминговом сервисе Spotify.

Далее определено две эвристики:
1. Пользователь более вероятно будет слушать исполнителей из одного жанра, чем из разных.
2. Более популярные исполнители должны чаще встречаться в базе транзакций.

На их основе составлена матрица вероятностей (Рисунок @probs), где в каждой столбце указано число, показывающее, с какой вероятностью следующим исполнителем будет исполнитель на указанной строке - чем больше это число, тем выше вероятность. Сама матрица не нормализована, однако при генерации данных все вероятности автоматически нормализуются, чтобы быть в интервале $[0,1]$.

#figure(image("/img/holm-prac1/holm-prac1_2025-03-14-07-12-14.png"), caption: [Матрица вероятностей перехода к следующему исполнителю])<probs>

Сама генерация состоит из нескольких этапов:
1. Псевдослучайным образом выбирается количество элементов в транзакции.
2. Псевдослучайным образом выбирается первый элемент транзакции.
3. На основе указанных выше вероятностей выбираются следующие элементы.

Код генерации представлен в Листинге @generate.

= Практическая часть
Для всех алгоритмов задан одинаковый минимальный порог поддержки: 4.
== Реализация алгоритма Apriori
Для алгоритма Apriori множество транзакций представлено в нормализованном виде (Рисунок @norm-df).

#figure(image("/img/holm-prac1/holm-prac1_2025-03-14-07-57-54.png"), caption: [Нормализованная матрица])<norm-df>

После выполнения самого алгоритма получено много частых наборов, которые затем были переведены в правила (Рисунок @apriori-rules).

#figure(image("/img/holm-prac1/holm-prac1_2025-03-14-08-01-38.png", height: 30%), caption: [Правила, полученные алгоритмом Apriori])<apriori-rules>

Далее из этих правил были выбраны те, у которых значение лифта больше единицы, а значение достоверности больше 30% (Рисунок @apriori-filter-rules)

#figure(image("/img/holm-prac1/holm-prac1_2025-03-14-08-04-10.png"), caption: [Отфильтрованные правила])<apriori-filter-rules>

Код реализации алгоритма представлен в Листинге @apriori-code.

== Реализация алгоритма Eclat
Для алгоритма Eclat для каждого элемента составлено множество транзакций (Рисунок @eclat-df).

#figure(image("/img/holm-prac1/holm-prac1_2025-03-14-08-06-07.png"), caption: [Изменённый вид датасета])<eclat-df>

После выполнения самого алгоритма получено много частых наборов, которые затем были переведены в правила (Рисунок @eclat-rules).

#figure(image("/img/holm-prac1/holm-prac1_2025-03-14-08-07-26.png"), caption: [Правила, полученные алгоритмом Eclat])<eclat-rules>

Далее из этих правил были выбраны те, у которых значение лифта больше единицы, а значение достоверности больше 30% (Рисунок @eclat-filter-rules-1 - @eclat-filter-rules-2).

#figure(image("/img/holm-prac1/holm-prac1_2025-03-14-08-08-31.png", height: 40%), caption: [Отфильтрованные правила])<eclat-filter-rules-1>
#figure(image("/img/holm-prac1/holm-prac1_2025-03-14-08-09-08.png", height: 40%), caption: [Отфильтрованные правила])<eclat-filter-rules-2>

Код реализации алгоритма представлен в Листинге @eclat-code.

== Реализация алгоритма FP-Growth
В начале элементы в каждой транзакции отсортированы по убыванию частотности (Рисунок @fp-sort)

#figure(image("/img/holm-prac1/holm-prac1_2025-03-14-08-12-47.png"), caption: [Время выполнения сортировки элементов])<fp-sort>

Затем выполнено построение дерева из элементов (Рисунок @fp-tree) --- дерево представлено строками из элементов, где записан сам узел и список его детей (узел None соответствует пустому корневому узлу).

#figure(image("/img/holm-prac1/holm-prac1_2025-03-14-08-14-44.png"), caption: [Построенное дерево])<fp-tree>

После выполнения самого алгоритма получено много частых наборов, которые затем были переведены в правила (Рисунок @fp-rules).

#figure(image("/img/holm-prac1/holm-prac1_2025-03-14-08-17-51.png"), caption: [Правила, полученные алгоритмом FP-Growth])<fp-rules>

Далее из этих правил были выбраны те, у которых значение лифта больше единицы, а значение достоверности больше 30% (Рисунок @fp-filter-rules).

#figure(image("/img/holm-prac1/holm-prac1_2025-03-14-08-21-03.png"), caption: [Отфильтрованные правила])<fp-filter-rules>

Код реализации алгоритма представлен в Листинге @fp-code.


#heading([Заключение], numbering: none)
В результате реализации алгоритмов выяснено, что самым быстрым алгоритмом является Eclat, поскольку нормализация данных под него занимает мало времени, в отличие от Apriori и FP-Growth. В то же время, FP_Growth является самым экономным в плане памяти за счёт того, что единственной структурой, которую нужно держать в памяти, является легковесное дерево, в отличие от массивов и множеств в Apriori и Eclat. Также FP-Growth выдал наибольшее количество результатов.

#bibliography("authors1.bib", full: true, style: "/src/gost-r-7-0-5-2008-numeric-alphabetical.csl", title: "Список использованных источников")

#appendix()

#set heading(offset: 5)
= Генерация данных
#simple-code(raw(read("prac1/generate.py")), [Генерация данных], label: <generate>)
= Реализация алгоритма Apriori
#simple-code(raw(read("prac1/apriori.py")), [Алгоритм Apriori], label: <apriori-code>)
= Реализация алгоритма Eclat
#simple-code(raw(read("prac1/eclat.py")), [Алгоритм Eclat], label: <eclat-code>)
= Реализация алгоритма FP-Growth
#simple-code(raw(read("prac1/fpgrowth.py")), [Алгоритм FP-Growth], label: <fp-code>)